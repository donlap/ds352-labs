{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical Learning for Data Science 2 (229352)\n",
        "#### Instructor: Donlapark Ponnoprat\n",
        "\n",
        "#### [Course website](https://donlapark.pages.dev/229352/)\n",
        "\n",
        "## Lab #12"
      ],
      "metadata": {
        "id": "Mhy_zXOqQYgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: Debug with CPU first, then run the whole notebook with GPU"
      ],
      "metadata": {
        "id": "c1KXqsZ6QeGC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMoXdgLYHmVc"
      },
      "outputs": [],
      "source": [
        "!pip install pythainlp\n",
        "!wget http://www.donlapark.cmustat.com/229352/thai_lyrics.tar.xz\n",
        "!tar xf thai_lyrics.tar.xz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import csv\n",
        "from itertools import chain\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from pythainlp import word_tokenize"
      ],
      "metadata": {
        "id": "XKXu1C_c_JNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4htrTvCOhPBD"
      },
      "source": [
        "# GPT for song lyrics generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQ51vFVH8uhg"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('thai_lyrics.csv', engine='python')\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set hyperparameters"
      ],
      "metadata": {
        "id": "ZzMFp-zNaW4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "LEARNING_RATE = 0.0003\n",
        "BATCH_SIZE = 12  # 128 for GPU\n",
        "NUM_EPOCHS = 5\n",
        "max_len = 64  # 128 for GPU  # Max sequence length\n",
        "d_model = 32  # 128  # Model dimensionality\n",
        "num_heads = 4       # Number of attention heads\n",
        "num_layers = 4  # 6 for GPU  # Number of transformer blocks\n",
        "hidden_dim = 128  # 512 for GPU  # Hidden dimension in feedforward network\n",
        "dropout_rate = 0.1\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "YMVxqwdOaWUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEg_FFZcg2hn"
      },
      "source": [
        "### Convert from words to numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laZhsugj40XF"
      },
      "outputs": [],
      "source": [
        "#[[song , number , one],[song , number , two]] -> [song , number , one , song , number , two]\n",
        "def flatten(ls):\n",
        "    \"\"\"\n",
        "    Flatten list of list\n",
        "    \"\"\"\n",
        "    return list(chain.from_iterable(ls))\n",
        "\n",
        "#[song , number ,one, number, two] -> [1,2,3,2,4] and [1,2,3] -> [song , number , one]\n",
        "def create_lookup_dict(tokenized_lyrics, n_min=None):\n",
        "    \"\"\"\n",
        "    Create lookup dictionary from list of words (lyrics)\n",
        "    \"\"\"\n",
        "    word_counts = Counter(tokenized_lyrics)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    if n_min is not None:\n",
        "        sorted_vocab = {k: v for k, v in word_counts.items() if v >= n_min}\n",
        "    vocab_to_int = {word: i for i, word in enumerate(sorted_vocab, 0)}\n",
        "    int_to_vocab = {i: word for word, i in vocab_to_int.items()}\n",
        "    return (vocab_to_int, int_to_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7h76Ddh5pxy"
      },
      "outputs": [],
      "source": [
        "df = df.iloc[:10, :]  # df.iloc[:1000, :] for GPU\n",
        "tokenized_lyrics = df['lyrics'].map(word_tokenize)\n",
        "tokenized_lyrics = flatten(tokenized_lyrics)\n",
        "tokenized_lyrics = [token if token != '\\n' else ' ' for token in tokenized_lyrics]\n",
        "word_counts = Counter(tokenized_lyrics)\n",
        "vocab_to_int, int_to_vocab = create_lookup_dict(tokenized_lyrics, n_min=None)\n",
        "vocab_size = len(vocab_to_int)  # number of words in lyrics corpus\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgUDR6WZgqCi"
      },
      "source": [
        "### Create Features (previous 50 words) and Target (Word 1-51)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaH7s1JS6rXB"
      },
      "outputs": [],
      "source": [
        "tokenized_indices = [vocab_to_int.get(token, 0) for token in tokenized_lyrics]\n",
        "\n",
        "X, target = [], []\n",
        "for n in range(0, len(tokenized_indices) - max_len, 1):\n",
        "  x = tokenized_indices[n: n + max_len]\n",
        "  y = tokenized_indices[n + 1: n + max_len + 1]  # output length = input length\n",
        "  X.append(np.array(x))\n",
        "  target.append(y)\n",
        "X = np.array(X)\n",
        "target = np.array(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M42FcfW2LgiH"
      },
      "outputs": [],
      "source": [
        "class MyDataSet(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    super(MyDataSet, self).__init__()\n",
        "    self._X = X\n",
        "    self._y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self._X.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    X = self._X[index]\n",
        "    y = self._y[index]\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9FTHz2eLqpW"
      },
      "outputs": [],
      "source": [
        "dataset = MyDataSet(X, target)\n",
        "\n",
        "trainloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxaVQAwqUTwl"
      },
      "source": [
        "## Transformer decoder\n",
        "\n",
        "<center><img src=\"https://donlapark.pages.dev/229352/Full-GPT-arch.png\" alt=\"GPT\" width=\"700\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Fill in the code blocks with `TODO ` tag in order to complete the GPT model."
      ],
      "metadata": {
        "id": "km8L58QPJQTv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4L6bwtBUZnw"
      },
      "source": [
        "### Positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sin to even indices (2i)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cos to odd indices (2i+1)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))  # Shape: (1, max_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]"
      ],
      "metadata": {
        "id": "B8fv6rEdIZ3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPuELnL_UnsX"
      },
      "source": [
        "### Self-Attention (`TODO:` Complete the self-attention block)\n",
        "\n",
        "<center><img src=\"https://donlapark.pages.dev/229352/self-attention-matrix-calculation.png\" alt=\"GPT\" width=\"600\"/></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Masked attention\n",
        "\n",
        "<center><img src=\"https://donlapark.pages.dev/229352/masked-attention.png\" alt=\"GPT\" width=\"500\"/></center>"
      ],
      "metadata": {
        "id": "CB1Lapla1Mrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = 5\n",
        "\n",
        "# Add this to your QKᵀ matrix *before* Softmax\n",
        "torch.triu(torch.full((T, T), float(\"-inf\")).to(device), diagonal=1)"
      ],
      "metadata": {
        "id": "AGlKuFjDzDVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout_rate):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        # Linear projections for query, key, and value\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.att_dropout = nn.Dropout(dropout_rate)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # Batch size, Time (seq length), Embedding size (d_model)\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.query(x)\n",
        "        k = #\n",
        "        v = #\n",
        "\n",
        "        # Split into multiple heads (C = num_heads * head_dim)\n",
        "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # [B, num_heads, T, head_dim]\n",
        "        print(q.shape)\n",
        "        k = #\n",
        "        v = #\n",
        "\n",
        "        \"\"\"\n",
        "        Calculate the self-attention with these steps:\n",
        "        1. Q @ Kᵀ: (B, num_heads, T, head_dim) @ (B, num_heads, head_dim, T)ᵀ -> (B, num_heads, T, T)\n",
        "        2. Normalize with sqrt(head_dim) (or head_dim**0.5)\n",
        "        3. Add attention mask\n",
        "        4. Apply Softmax\n",
        "        5. Apply Dropout\n",
        "        6. Multiply by V (use @): (B, num_heads, T, T) @ (B, num_heads, T, head_dim) -> (B, num_heads, T, head_dim)\n",
        "        7. Transpose then Reshape to (B, T, num_heads*head_dim)\n",
        "        8. Apply the final linear layer (self.out_proj)\n",
        "        Note: If you got `RuntimeError: input is not contiguous`,\n",
        "              call the `.contiguous()` method **after transposing** your output.\n",
        "        \"\"\"\n",
        "        return out"
      ],
      "metadata": {
        "id": "4JnoYKX1InB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feedforward block\n",
        "\n",
        "<center><img src=\"https://donlapark.pages.dev/229352/Feedforward-block.png\" alt=\"GPT\" width=\"400\"/></center>"
      ],
      "metadata": {
        "id": "8iZoxHphIo5Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZivdouMU0F2"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, d_model)\n",
        "        self.gelu = nn.GELU()  # Activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.gelu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Decoder block (`TODO:` Fill in code)\n",
        "\n",
        "<center><img src=\"https://donlapark.pages.dev/229352/GPT-Decoder.png\" alt=\"GPT\" width=\"400\"/></center>"
      ],
      "metadata": {
        "id": "6tJ0ZhWuI5zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, hidden_dim, dropout_rate):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(d_model, num_heads, , dropout_rate)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.feedforward = FeedForward(d_model, hidden_dim)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #TODO: Your code here\n",
        "        return out"
      ],
      "metadata": {
        "id": "bDToNb0aI5a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT\n",
        "\n",
        "<center><img src=\"https://donlapark.pages.dev/229352/GPT.png\" alt=\"GPT\" width=\"150\"/></center>\n",
        "\n",
        "**Note:** The `CrossEntropyLoss` requires output shape = `(batch_size, vocab_size, seq_length)`. Make sure that your output matches this shape!"
      ],
      "metadata": {
        "id": "ID3WMHsrI8jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len, d_model, num_heads, num_layers, hidden_dim, dropout_rate):\n",
        "        super(GPT, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(max_len, d_model)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, hidden_dim, dropout_rate) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(d_model)  # Final layer normalization\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)  # Output layer (vocab_size classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #TODO: Your code here\n",
        "        return out"
      ],
      "metadata": {
        "id": "y8-Du-R_JE7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpZ_4U85IqcB"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = GPT(\n",
        "    vocab_size,\n",
        "    max_len,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    num_layers,\n",
        "    hidden_dim,\n",
        "    dropout_rate\n",
        "    ).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPALHPWyeH_V"
      },
      "source": [
        "### Exercise 2: fill in the code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLRparXRIrZ9"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def generate(model, start_word, pad_value=0, predict_len=200):\n",
        "    # Tokenize the input sentence\n",
        "    words = word_tokenize(start_word)\n",
        "    start_word_ids = []\n",
        "    # List to store the predictions\n",
        "    predicted = words\n",
        "\n",
        "    # Words -> Integers\n",
        "    word_ids = [vocab_to_int.get(word, pad_value) for word in words]\n",
        "\n",
        "    #[28,15] -> [0,0,28,15]\n",
        "    current_seq = [np.pad(word_ids, (max_len - len(word_ids) - 1, pad_value), 'constant')]\n",
        "\n",
        "    for _ in range(predict_len):\n",
        "        current_seq = torch.LongTensor(np.array(current_seq)).to(device)\n",
        "        # get the next word probabilities\n",
        "        p = model(current_seq)[:, :, -1]\n",
        "        p = nn.Softmax(dim=1)(p).cpu().detach().numpy()\n",
        "        # p = [[0.1,0.2,0.05,0.03,0.02,0.3,0.2,0.1]]\n",
        "        p = p[0]\n",
        "        # p = [0.1,0.2,0.05,0.03,0.02,0.3,0.2,0.1]\n",
        "\n",
        "        # Sample from probability distribution p\n",
        "        word_i = np.random.choice(np.arange(0, p.shape[0]), p=p)\n",
        "        predicted.append(int_to_vocab[word_i])\n",
        "\n",
        "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
        "        current_seq = current_seq.cpu().detach().numpy()\n",
        "        current_seq = np.roll(current_seq, -1, 1)\n",
        "        current_seq[-1][-1] = word_i\n",
        "    gen_sentences = ''.join(predicted)\n",
        "    return gen_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-9hflaPec0j"
      },
      "source": [
        "### Exercise 3: use `generate` function to generate new text for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GInZpHdqI8p2"
      },
      "outputs": [],
      "source": [
        "pad_int = vocab_to_int[' ']\n",
        "\n",
        "for t in range(NUM_EPOCHS):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(trainloader, model, loss_fn, optimizer)\n",
        "    with torch.no_grad():\n",
        "      print(generate(model, 'ฉันก็',\n",
        "                     pad_value=pad_int, predict_len=200))\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra: Using Transformers Library"
      ],
      "metadata": {
        "id": "R1Gf-2d2yWcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers Documentations: https://huggingface.co/docs/transformers/index"
      ],
      "metadata": {
        "id": "R1nD-V3v_RWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequence Classification"
      ],
      "metadata": {
        "id": "dJwL2hUwyiXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(task=\"sentiment-analysis\",\n",
        "                      model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
      ],
      "metadata": {
        "id": "RWKOuf1Ayc-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\"I love to hate you\")"
      ],
      "metadata": {
        "id": "Mo13eeQTyomd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A closer look: Tokenization + Classification"
      ],
      "metadata": {
        "id": "yClwj6gV_aKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load tokenizer"
      ],
      "metadata": {
        "id": "M4DMa-JJ_fWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "L3bOkrck_Zol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenize"
      ],
      "metadata": {
        "id": "ZzW0DKPi_mb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love you\"\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "tokens"
      ],
      "metadata": {
        "id": "lOEdNdtC_dSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert tokens to ids"
      ],
      "metadata": {
        "id": "gxY41WdK_pJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "sentence"
      ],
      "metadata": {
        "id": "YrLvMRao_s-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert from sentence to ids directly"
      ],
      "metadata": {
        "id": "XePzIxH8AHQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = tokenizer(text,  return_tensors=\"pt\")\n",
        "\n",
        "sentence"
      ],
      "metadata": {
        "id": "HuXfQiDhAOcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use the model to classify on the input ids"
      ],
      "metadata": {
        "id": "F-Aa9M-tAUmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "model(**sentence).logits"
      ],
      "metadata": {
        "id": "jVRtvz1PAYaW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}