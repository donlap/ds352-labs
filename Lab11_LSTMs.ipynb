{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMoXdgLYHmVc"
      },
      "outputs": [],
      "source": [
        "!pip install pythainlp\n",
        "!wget http://www.donlapark.cmustat.com/229352/thai_lyrics.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4htrTvCOhPBD"
      },
      "source": [
        "#Song lyrics generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQ51vFVH8uhg"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import csv\n",
        "from itertools import chain\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pythainlp import word_tokenize\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "df = pd.read_csv('thai_lyrics.csv', engine='python')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF8se3vr9IE6"
      },
      "outputs": [],
      "source": [
        "tokenized_lyrics = df['lyrics'].map(word_tokenize)\n",
        "print(tokenized_lyrics[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEg_FFZcg2hn"
      },
      "source": [
        "### Convert from words to numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laZhsugj40XF"
      },
      "outputs": [],
      "source": [
        "\n",
        "#[[song , number , one],[song , number , two]] -> [song , number , one , song , number , two]\n",
        "def flatten(ls):\n",
        "    \"\"\"\n",
        "    Flatten list of list\n",
        "    \"\"\"\n",
        "    return list(chain.from_iterable(ls))\n",
        "\n",
        "#[song , number ,one, number, two] -> [1,2,3,2,4] and [1,2,3] -> [song , number , one]\n",
        "def create_lookup_dict(tokenized_lyrics, n_min=None):\n",
        "    \"\"\"\n",
        "    Create lookup dictionary from list of words (lyrics)\n",
        "    \"\"\"\n",
        "    word_counts = Counter(tokenized_lyrics)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    if n_min is not None:\n",
        "        sorted_vocab = {k: v for k, v in word_counts.items() if v >= n_min}\n",
        "    vocab_to_int = {word: i for i, word in enumerate(sorted_vocab, 0)}\n",
        "    int_to_vocab = {i: word for word, i in vocab_to_int.items()}\n",
        "    return (vocab_to_int, int_to_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7h76Ddh5pxy"
      },
      "outputs": [],
      "source": [
        "tokenized_lyrics = flatten(tokenized_lyrics)\n",
        "#tokenized_lyrics = [token if token is not '\\n' else ' ' for token in tokenized_lyrics]\n",
        "word_counts = Counter(tokenized_lyrics)\n",
        "vocab_to_int, int_to_vocab = create_lookup_dict(tokenized_lyrics, n_min=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i49_sKArgkeQ"
      },
      "outputs": [],
      "source": [
        "vocab_to_int[\"ใคร\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji5YtYv2vqq4"
      },
      "outputs": [],
      "source": [
        "len(vocab_to_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDeFF62ngkmN"
      },
      "outputs": [],
      "source": [
        "int_to_vocab[12]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgUDR6WZgqCi"
      },
      "source": [
        "### Create Features (20 words in a song) and Target (the next word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaH7s1JS6rXB"
      },
      "outputs": [],
      "source": [
        "sequence_length = 20\n",
        "\n",
        "tokenized_indices = [vocab_to_int.get(token, 0) for token in tokenized_lyrics]\n",
        "\n",
        "X, target = [], []\n",
        "for n in range(0, len(tokenized_indices) - sequence_length, 1):\n",
        "  x = tokenized_indices[n: n + sequence_length]\n",
        "  y = tokenized_indices[n + sequence_length]\n",
        "  X.append(np.array(x))\n",
        "  target.append(y)\n",
        "X = np.array(X)\n",
        "target = np.array(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP_j2yBfgjEP"
      },
      "outputs": [],
      "source": [
        "X[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Re-6oe-gjdB"
      },
      "outputs": [],
      "source": [
        "target[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M42FcfW2LgiH"
      },
      "outputs": [],
      "source": [
        "class MyDataSet(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    super(MyDataSet, self).__init__()\n",
        "    self._X = X\n",
        "    self._y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self._X.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    X = self._X[index]\n",
        "    y = self._y[index]\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9FTHz2eLqpW"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Classification\n",
        "NUM_CLASSES = len(vocab_to_int)\n",
        "\n",
        "dataset = MyDataSet(X, target)\n",
        "\n",
        "trainloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxaVQAwqUTwl"
      },
      "source": [
        "## New layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4L6bwtBUZnw"
      },
      "source": [
        "## 1. `nn.Embedding(num_vocabs, hidden_dim)`\n",
        "\n",
        "[PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "\n",
        "![emb](https://miro.medium.com/max/720/1*NuWIU2Iew3Bm8NR78tRj8A.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IxIMSJtUs3Y"
      },
      "outputs": [],
      "source": [
        "embedding = nn.Embedding(num_embeddings=10, embedding_dim=3)\n",
        "# a batch of 2 samples of 4 indices each\n",
        "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
        "output = embedding(input)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPuELnL_UnsX"
      },
      "source": [
        "## 2. LSTM\n",
        "[PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMDmCYN6Svkb"
      },
      "source": [
        "![lstm](https://i.stack.imgur.com/sBEBp.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZivdouMU0F2"
      },
      "outputs": [],
      "source": [
        "lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2)\n",
        "input = torch.randn(5, 3, 10)\n",
        "h0 = torch.randn(2, 3, 20)  # initial hidden state\n",
        "c0 = torch.randn(2, 3, 20)  # initial cell state\n",
        "output, (h1, c1) = lstm(input, (h0, c0))\n",
        "\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwDMjEUPdn6Z"
      },
      "source": [
        "### Exercise 1: fill in the code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOXkBy9ENokC"
      },
      "outputs": [],
      "source": [
        "class Simple_LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Simple_LSTM, self).__init__()\n",
        "\n",
        "        # TODO: Fill in the layers' parameters. Suggested hidden dimensions: 64, 128, 256, 512\n",
        "        self.embeddings = nn.Embedding(num_embeddings=####, embedding_dim=####)\n",
        "        self.lstm = nn.LSTM(input_size=####, hidden_size=####, dropout = 0.2, num_layers=####)\n",
        "        self.fc = nn.Linear(####, ####)  # Hint: predicting the next word is a classification problem with num_vocabs classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # for LSTM, input should be (Sequnce_length, batch_size, hidden_layer),\n",
        "        # so we need to transpose the input\n",
        "        x = x.t()\n",
        "        # Apply the Embedding layer\n",
        "        x = self.embeddings(x)\n",
        "        # Apply the LSTM layer (note: LSTM's output is a tuple!)\n",
        "        h, _ = self.lstm(x)\n",
        "        # Only need to keep the last element of the sequence\n",
        "        ht=h[-1]\n",
        "        out = self.fc(ht)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpZ_4U85IqcB"
      },
      "outputs": [],
      "source": [
        "model = Simple_LSTM().to('cuda')\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPALHPWyeH_V"
      },
      "source": [
        "### Exercise 2: The `generate` functions is used to generate full text from user's starting words (`start_word`)\n",
        "\n",
        "### Complete the code in `TODO#1` and `TODO#2` in the `generate ` function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLRparXRIrZ9"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        X = X.to('cuda')\n",
        "        y = y.to('cuda')\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 1000 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, start_word, int_to_vocab, vocab_to_int, predict_len=100):\n",
        "\n",
        "    words = word_tokenize(start_word)\n",
        "    start_word_ids = []\n",
        "\n",
        "    predicted = words  # we will append new words to this list\n",
        "\n",
        "    pad_value = vocab_to_int[\" \"]\n",
        "    word_ids = [vocab_to_int.get(word, pad_value) for word in words]\n",
        "\n",
        "    # Pad with zeros Ex: [28,15,16] -> [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,28,15,16]\n",
        "    current_seq = [np.pad(word_ids, (20 - len(word_ids), pad_value), 'constant')]\n",
        "\n",
        "    for _ in range(predict_len):\n",
        "        # transform the array of words into a tensor\n",
        "        current_seq = torch.LongTensor(np.array(current_seq)).to('cuda')\n",
        "        ############### TODO#1: Fill in the following steps##############\n",
        "        # 1. With the trained model, use current_seq as input and obtain the output\n",
        "        # 2. Apply the Softmax function (nn.Softmax) to turn the output from step 1 into a vector of probabilities.\n",
        "        #    nn.Softmax Documentation: https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
        "        # 3. Make sure that the output has shape (NUM_CLASSES,). Name the output p.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        p =\n",
        "        ############################end#################################\n",
        "\n",
        "        # top-k sampling\n",
        "        topk_probs, _ = torch.topk(p, k=100)\n",
        "        kth_prob = topk_probs[-1]\n",
        "        p = torch.where(p < kth_prob,\n",
        "                        torch.full_like(p, 0), p)\n",
        "        p /= p.sum()\n",
        "        p = p.cpu().detach().numpy()\n",
        "\n",
        "        # Sample from probability distribution p\n",
        "        # word_i is an integer representing a word.\n",
        "        word_i = np.random.choice(np.arange(0,p.shape[0]), p=p)\n",
        "\n",
        "        ############### TODO#2: Fill in the following code##############\n",
        "        # 1. Convert from word_i (int)--> word (str)\n",
        "        # 2. Append the word from 1. into the `predicted` list defined above.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ############################end#################################\n",
        "\n",
        "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
        "        current_seq = current_seq.cpu().detach().numpy()\n",
        "        current_seq = np.roll(current_seq, -1, 1)\n",
        "        current_seq[-1][-1] = word_i\n",
        "    gen_sentences = ''.join(predicted)\n",
        "    return gen_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-9hflaPec0j"
      },
      "source": [
        "### Exercise 3: use `generate` function to generate three more songs. You may try using different starting words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GInZpHdqI8p2"
      },
      "outputs": [],
      "source": [
        "for t in range(NUM_EPOCHS):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(trainloader, model, loss_fn, optimizer)\n",
        "    print(generate(model, 'วันที่ฉันเดินอยู่คนเดียว', int_to_vocab, vocab_to_int, predict_len=100))\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmwrYv8MImyY"
      },
      "source": [
        "# Extra: Web scraping with Beautiful soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Stitl9W-IpXf"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "\n",
        "def scrape_siamzone_url(d):\n",
        "    \"\"\"\n",
        "    Script to scrape Siamzone lyrics from a given song_id (integer)\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(requests.get('https://www.siamzone.com/music/thailyric/{}'.format(d)).content, 'html.parser')\n",
        "    song_title, artist_name = soup.find('title').text.split('|')\n",
        "    song_title, artist_name = song_title.replace(\"เนื้อเพลง \", \"\").strip(), artist_name.strip()\n",
        "    try:\n",
        "        n_views = ' '.join(soup.find('div', attrs={'class': 'has-text-info'}).text.strip().split())\n",
        "    except:\n",
        "        n_views = ''\n",
        "    try:\n",
        "        full_lyrics = soup.find_all('div', attrs={'class': 'column is-6-desktop'})[1]\n",
        "        lyrics = full_lyrics.find(\"div\", attrs={'style': \"margin-bottom: 1rem;\"}).text.strip()\n",
        "    except:\n",
        "        lyrics = \"\"\n",
        "    return {\n",
        "        'url': 'https://www.siamzone.com/music/thailyric/%d' % d,\n",
        "        'soup': soup,\n",
        "        'song_title': song_title,\n",
        "        'artist_name': artist_name,\n",
        "        'n_views': n_views,\n",
        "        'lyrics': lyrics\n",
        "    }\n",
        "\n",
        "def clean_lyrics(lyric):\n",
        "    \"\"\"\n",
        "    Clean lines that do not contain lyrics\n",
        "    \"\"\"\n",
        "    lines = lyric.split('\\n')\n",
        "    lyrics_clean = []\n",
        "    for line in lines:\n",
        "        # remove headers from the file\n",
        "        headers = [\n",
        "            'เพลง ', 'คำร้อง ', 'คำร้อง/ทำนอง ', 'ศิลปิน ', 'ทำนอง ',\n",
        "            'เรียบเรียง ', 'เพลงประกอบละคร ', 'อัลบัม ', 'ร่วมร้องโดย ',\n",
        "            'เนื้อร้อง/ทำนอง', 'ทำนอง/เรียบเรียง ', 'เพลงประกอบภาพยนตร์ ',\n",
        "            'เพลงประกอบละครซิทคอม ', 'คำร้อง/ทำนอง/เรียบเรียง ',\n",
        "            'คำร้อง/เรียบเรียง ', 'เพลงประกอบ ', 'ร้องโดย ',\n",
        "            'ทำนอง / เรียบเรียง :', ' สังกัด'\n",
        "        ]\n",
        "        if any(line.startswith(s) for s in headers):\n",
        "            pass\n",
        "        else:\n",
        "            line = ' '.join(line.replace('(', ' ').replace(')', ' ').replace('-', ' ').split())\n",
        "            lyrics_clean.append(line)\n",
        "    return '\\n'.join(lyrics_clean).strip()\n",
        "\n",
        "def scrape_siamzone():\n",
        "    data = []\n",
        "    for i in range(23649, 28649):\n",
        "        try:\n",
        "            data.append(scrape_siamzone_url(i))\n",
        "        except:\n",
        "            pass\n",
        "        if i % 100 == 0:\n",
        "            print(i)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df['lyrics'] = df['lyrics'].map(clean_lyrics)\n",
        "    return df"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}