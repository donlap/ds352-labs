{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donlap/ds352-labs/blob/main/Lab11_Gemma3_with_Unsloth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKnOvZX3UkRk"
      },
      "source": [
        "### Statistical Learning for Data Science 2 (229352)\n",
        "#### Instructor: Donlapark Ponnoprat\n",
        "\n",
        "#### [Course website](https://donlapark.pages.dev/229352/)\n",
        "\n",
        "## Lab #11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSVz_-DHNhEP"
      },
      "source": [
        "# Finetuning a Gemma-3 Model for Text Classification with Unsloth\n",
        "\n",
        "Today, you will learn how to take a pre-trained Large Language Model (LLM) and specialize it for **text classification**.\n",
        "\n",
        "We will be using [**Unsloth**](https://docs.unsloth.ai/get-started/all-our-models) to speeds up finetuning and reduces memory usage, making it possible to train in Google Colab.\n",
        "\n",
        "**Goal:** Finetune the `Gemma-3-1B` model on the `wisesight_sentiment` dataset to classify Thai text into one of four categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "jX-WcFwjUtHp"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install torch==2.6.0 torchvision=0.21.0  # don't need this for Colab\n",
        "%pip install transformers>=4.52.4\n",
        "%pip install --no-deps bitsandbytes xformers==0.0.29.post3\n",
        "%pip install git+https://github.com/donlap/unsloth-zoo.git@patch/skip-no-quant-state\n",
        "%pip install datasets\n",
        "%pip install git+https://github.com/donlap/unsloth.git@feature/sequence_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrQPEJIPUtHs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "print(f\"Major: {major_version}, Minor: {minor_version}\")\n",
        "from datasets import load_dataset\n",
        "import datasets\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.models import gemma3_sequence_classification\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from peft import TaskType\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import warnings\n",
        "from typing import Any, Dict, List, Tuple, Union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSwrCMlMOUTH"
      },
      "source": [
        "## Load and Prepare the Dataset\n",
        "\n",
        "We'll use the `wisesight_sentiment` dataset, which contains Thai text labeled with one of four sentiment categories (positive, negative, neutral, question). We'll rename the columns to `text` and `label` to match what the `Trainer` expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o713IRq9UtHw"
      },
      "outputs": [],
      "source": [
        "# Load the Wisesight Sentiment dataset\n",
        "dataset = load_dataset(\"wisesight_sentiment\")\n",
        "for set_name in dataset:\n",
        "    dataset[set_name] = dataset[set_name].rename_column(\"texts\", \"text\")\n",
        "    dataset[set_name] = dataset[set_name].rename_column(\"category\", \"label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_znUiLFxOmlP"
      },
      "source": [
        "## Configure the Model and Tokenizer\n",
        "\n",
        "In the following code block, we will:\n",
        "1.  Define our model parameters.\n",
        "2.  Load a 4-bit quantized version of `unsloth/gemma-3-1b-it-unsloth-bnb-4bit` using `FastLanguageModel`. Quantization reduces the model's memory footprint significantly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVnMGUcwOuEu"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 4 # number of classes in the csv\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "\n",
        "model_name = \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\" #\"unsloth/Llama-3.2-1B-bnb-4bit\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    auto_model = AutoModelForSequenceClassification,\n",
        "    num_labels = NUM_CLASSES,\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        "    load_in_8bit = False,\n",
        "    full_finetuning = False,\n",
        ")\n",
        "model.score = torch.nn.Linear(\n",
        "    model.score.weight.shape[1], NUM_CLASSES, bias=False, device=model.device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQMiTsg_O-Ne"
      },
      "source": [
        "3.  Configure **LoRA (Low-Rank Adaptation)**, a parameter-efficient finetuning (PEFT) technique. Instead of training all the model's billions of parameters, we only train a small number of \"adapter\" layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eiifan4mUtHv"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    max_seq_length = max_seq_length,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    r = 8,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 8,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,  # Supports rank stabilized LoRA\n",
        "    task_type = TaskType.SEQ_CLS # Sequence to Classification Task\n",
        ")\n",
        "\n",
        "print(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdY7MnbIPKGb"
      },
      "source": [
        "## Finetune the Model\n",
        "\n",
        "Now, we'll set up the `Trainer` from the HuggingFace's `transformers` library. This class handles the entire training loop, including batching, gradient updates, and logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ6fr79zUtHx"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"])\n",
        "\n",
        "\n",
        "train_tokenized = dataset['train'].map(tokenize_function, batched=True)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    train_dataset = train_tokenized,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 32,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        warmup_steps = 10,\n",
        "        learning_rate = 1e-4,\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        num_train_epochs = 1,\n",
        "        report_to = \"none\",\n",
        "        group_by_length = True,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lz7OiY-1UtHy"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sydT72FAPWim"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Here's an example of the model's prediction on a sample text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Suk5knPYUtHy"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
        "\n",
        "test_df = dataset['test'].to_pandas()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    text = test_df['text'].iloc[2]\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "    preds = model(**inputs).logits\n",
        "    print(text)\n",
        "    print(preds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqFRR4nBPiWW"
      },
      "source": [
        "## Exercise 1: Evaluate the Model\n",
        "\n",
        "Training is done,now it's time to evaluate your model on the test set, which is stored in a Pandas dataframe `test_df`.\n",
        "\n",
        "**Your Task:**\n",
        "1.  Iterate through the test data in batches. A batch size of 16 or 32 is a good choice.\n",
        "2.  For each batch:\n",
        "    *   Tokenize the texts. Make sure to add `padding=True` and `return_tensors=\"pt\"` to get a PyTorch tensor.\n",
        "    *   Move the tokenized inputs to the same device as the model (`model.device`).\n",
        "    *   Get the model's predictions (logits).\n",
        "    *   Find the predicted class for each text by taking the `argmax` of the logits.\n",
        "3.  Keep track of how many predictions are correct.\n",
        "4.  After the loop, calculate and print the final accuracy (in percent).\n",
        "5.  To inspect our predictions, print the `Text`, `True Label`, and `Predicted Label` for the first example in each batch.\n",
        "\n",
        "Fill in the code in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmx4p2I1UtHz"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# Free some memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Evaluation parameters. You can add more.\n",
        "batch_size = 32\n",
        "num_correct = 0\n",
        "\n",
        "with torch.inference_mode():  # Make predictions in this scope so that you won't accidentally modify the parameters.\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kswmjlamR3v9"
      },
      "source": [
        "## Exercise 2: Visualizing Model Attention\n",
        "\n",
        "How does the model decide on a classification? **Attention** is a key mechanism. It allows the model to weigh the importance of different words in the input text when making a prediction.\n",
        "\n",
        "By visualizing the attention matrix, we can get a glimpse into the model's \"thought process.\"\n",
        "\n",
        "**Your Task:**\n",
        "1.  Read through the `visualize_attention` helper function provided below. It handles the complex parts of extracting and plotting the attention weights.\n",
        "2.  Fill in the code in `Line 74` below to calculate $QK^T$, the Query-Key dot-product attention scores.\n",
        "3.  Choose your own text. Call the `visualize_attention` function with your text to see which words the model focuses on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLpniH_qSarI"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/google/fonts/raw/main/ofl/sarabun/Sarabun-Regular.ttf\n",
        "mpl.font_manager.fontManager.addfont('Sarabun-Regular.ttf')\n",
        "mpl.rc('font', family='Sarabun')\n",
        "plt.rcParams ['font.family'] = ('Sarabun')\n",
        "\n",
        "def visualize_manual_attention(text, layer=-1, head=0):\n",
        "    \"\"\"\n",
        "    Manually calculates and visualizes the attention matrix for a given text\n",
        "    using forward hooks to capture Q and K vectors. This method works even\n",
        "    after the model is optimized with `for_inference()`.\n",
        "\n",
        "    Args:\n",
        "        model: The finetuned model.\n",
        "        tokenizer: The tokenizer.\n",
        "        text (str): The input text to visualize.\n",
        "        layer (int): The model layer to visualize. Default is the last layer.\n",
        "        head (int): The attention head to visualize.\n",
        "    \"\"\"\n",
        "    # This dictionary will store the captured outputs\n",
        "    captured_outputs = {}\n",
        "\n",
        "    # Define a hook function factory\n",
        "    def get_hook(name):\n",
        "        def hook(model, input, output):\n",
        "            # For Q and K, the output is a tuple, we take the first element\n",
        "            captured_outputs[name] = output[0]\n",
        "        return hook\n",
        "\n",
        "    # Find the Q and K projection layers\n",
        "    target_layer = model.model.model.layers[layer]\n",
        "    q_proj_layer = target_layer.self_attn.q_proj\n",
        "    k_proj_layer = target_layer.self_attn.k_proj\n",
        "\n",
        "    # Register the forward hooks\n",
        "    q_hook_handle = q_proj_layer.register_forward_hook(get_hook('query'))\n",
        "    k_hook_handle = k_proj_layer.register_forward_hook(get_hook('key'))\n",
        "\n",
        "    # Prepare inputs\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=max_seq_length).to(model.device)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "    try:\n",
        "        # Run a forward pass to trigger the hooks\n",
        "        with torch.no_grad():\n",
        "            model(**inputs)\n",
        "\n",
        "        # Retrieve the captured Q and K tensors\n",
        "        Q = captured_outputs.get('query')\n",
        "        K = captured_outputs.get('key')\n",
        "\n",
        "        if Q is None or K is None:\n",
        "            print(\"Error: Failed to capture Q or K vectors. The hooks did not trigger correctly.\")\n",
        "            return\n",
        "\n",
        "        # The raw Q and K are of shape [batch, seq_len, hidden_dim].\n",
        "        # We need to reshape them based on the number of heads.\n",
        "        seq_len = len(tokens)\n",
        "\n",
        "        # From the Gemma-3 architecture, we know the head dimensions\n",
        "        num_q_heads = 4  # 1024 / 256\n",
        "        num_kv_heads = 1 # 256 / 256 (This is Grouped-Query Attention)\n",
        "        head_dim = 256\n",
        "\n",
        "        # Reshape Q and K to be per-head\n",
        "        Q_reshaped = Q.view(1, seq_len, num_q_heads, head_dim)\n",
        "        K_reshaped = K.view(1, seq_len, num_kv_heads, head_dim)\n",
        "\n",
        "        # Select the specific head for Q. In GQA, all Q heads attend to the same K head.\n",
        "        Q_head = Q_reshaped[0, :, head, :]  # Shape: [seq_len, head_dim]\n",
        "        K_head = K_reshaped[0, :, 0, :]     # Shape: [seq_len, head_dim]\n",
        "\n",
        "        # Calculate dot-product attention scores\n",
        "        # (Q * K^T)\n",
        "        attention_scores = ############# YOUR CODE HERE #############\n",
        "\n",
        "        # Scale the scores\n",
        "        # scores / sqrt(d_k)\n",
        "        scaled_scores = attention_scores / np.sqrt(head_dim)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = torch.nn.functional.softmax(scaled_scores, dim=-1).cpu().numpy()\n",
        "\n",
        "        # Plot the heatmap\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        plt.imshow(attention_weights, cmap='viridis', interpolation='nearest')\n",
        "        plt.colorbar()\n",
        "        plt.xticks(np.arange(len(tokens)), tokens, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "        plt.yticks(np.arange(len(tokens)), tokens)\n",
        "        plt.title(f'Attention Matrix - Layer {layer}, Head {head}')\n",
        "        plt.xlabel('Key/Memory Tokens')\n",
        "        plt.ylabel('Query/Input Tokens')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    finally:\n",
        "        # Remove the hooks to clean up\n",
        "        q_hook_handle.remove()\n",
        "        k_hook_handle.remove()\n",
        "\n",
        "\n",
        "#@title ✏️ Your Code for Visualization\n",
        "\n",
        "# 1. Pick a text from the test set\n",
        "example_text = \"สั่งของไปตั้งนานแล้ว ยังไม่มาส่งเลย แย่มาก\" # \"Ordered a long time ago, still not delivered. Very bad.\"\n",
        "\n",
        "# 2. Call the new visualization function\n",
        "# This will now work correctly regardless of whether `for_inference()` was called.\n",
        "visualize_manual_attention(example_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d68Lu1P_SoGr"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MukYU-D7SwPz"
      },
      "source": [
        "## Exercise 3: Conceptual Questions\n",
        "\n",
        "Please answer the following questions in the text cell provided.\n",
        "\n",
        "**Question 1: Pros and Cons of Finetuning**\n",
        "Based on this lab and your understanding, what are the pros and cons of finetuning a large pre-trained model compared to training a smaller model (e.g. logistic regression of SVM) from scratch for a specific task?\n",
        "\n",
        "**Question 2: LoRA Parameters**\n",
        "In Step 3, we configured LoRA with `r=16` and `lora_alpha=16`. Briefly explain the role of these two parameters. What might happen if you set `r` to a very high value (e.g., 256) for this small dataset?\n",
        "\n",
        "**Question 3: LLM Model Choice**\n",
        "Look through a [**list of models here**](https://docs.unsloth.ai/get-started/all-our-models). Name one model that you think might perform well when fine-tuned to the Thai text classification task. Why did you choose this model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtG2G-k3S0zL"
      },
      "source": [
        "**Answer 1:**\n",
        "\n",
        "\n",
        "\n",
        "**Answer 2:**\n",
        "\n",
        "\n",
        "\n",
        "**Answer 3:**\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}