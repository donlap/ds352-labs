{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donlap/ds352-labs/blob/main/Lab11_Gemma3_with_Unsloth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical Learning for Data Science 2 (229352)\n",
        "#### Instructor: Donlapark Ponnoprat\n",
        "\n",
        "#### [Course website](https://donlapark.pages.dev/229352/)\n",
        "\n",
        "## Lab #11"
      ],
      "metadata": {
        "id": "fKnOvZX3UkRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning a Gemma-3 Model for Text Classification with Unsloth\n",
        "\n",
        "Today, you will learn how to take a pre-trained Large Language Model (LLM) and specialize it for **text classification**.\n",
        "\n",
        "We will be using [**Unsloth**](https://docs.unsloth.ai/get-started/all-our-models) to speeds up finetuning and reduces memory usage, making it possible to train in Google Colab.\n",
        "\n",
        "**Goal:** Finetune the `Gemma-3-1B` model on the `wisesight_sentiment` dataset to classify Thai text into one of four categories."
      ],
      "metadata": {
        "id": "SSVz_-DHNhEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install transformers>=4.52.4\n",
        "%pip install --no-deps bitsandbytes xformers==0.0.29.post3\n",
        "%pip install git+https://github.com/donlap/unsloth-zoo.git@patch/skip-no-quant-state\n",
        "%pip install datasets\n",
        "%pip install git+https://github.com/donlap/unsloth.git@feature/sequence_classification"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T04:34:21.213540Z",
          "iopub.execute_input": "2025-06-18T04:34:21.214194Z",
          "iopub.status.idle": "2025-06-18T04:35:28.293284Z",
          "shell.execute_reply.started": "2025-06-18T04:34:21.214172Z",
          "shell.execute_reply": "2025-06-18T04:35:28.292456Z"
        },
        "id": "jX-WcFwjUtHp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "print(f\"Major: {major_version}, Minor: {minor_version}\")\n",
        "from datasets import load_dataset\n",
        "import datasets\n",
        "\n",
        "from unsloth import FastModel, FastLanguageModel, tokenizer_utils\n",
        "from unsloth.models import gemma3_sequence_classification\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import warnings\n",
        "from typing import Any, Dict, List, Tuple, Union\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T04:35:31.643116Z",
          "iopub.execute_input": "2025-06-18T04:35:31.643501Z",
          "iopub.status.idle": "2025-06-18T04:35:31.720975Z",
          "shell.execute_reply.started": "2025-06-18T04:35:31.643461Z",
          "shell.execute_reply": "2025-06-18T04:35:31.719949Z"
        },
        "id": "VrQPEJIPUtHs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Prepare the Dataset\n",
        "\n",
        "We'll use the `wisesight_sentiment` dataset, which contains Thai text labeled with one of four sentiment categories (positive, negative, neutral, question). We'll rename the columns to `text` and `label` to match what the `Trainer` expects."
      ],
      "metadata": {
        "id": "nSwrCMlMOUTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Wisesight Sentiment dataset\n",
        "dataset = load_dataset(\"wisesight_sentiment\")\n",
        "for set_name in dataset:\n",
        "    dataset[set_name] = dataset[set_name].rename_column(\"texts\", \"text\")\n",
        "    dataset[set_name] = dataset[set_name].rename_column(\"category\", \"label\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "o713IRq9UtHw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure the Model and Tokenizer\n",
        "\n",
        "In the following code block, we will:\n",
        "1.  Define our model parameters.\n",
        "2.  Load a 4-bit quantized version of `unsloth/gemma-3-1b-it-unsloth-bnb-4bit` using `FastLanguageModel`. Quantization reduces the model's memory footprint significantly."
      ],
      "metadata": {
        "id": "_znUiLFxOmlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 4 # number of classes in the csv\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "\n",
        "model_name = \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\" #\"unsloth/Llama-3.2-1B-bnb-4bit\"\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    auto_model = AutoModelForSequenceClassification,\n",
        "    num_labels = NUM_CLASSES,\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        "    load_in_8bit = False,\n",
        "    full_finetuning = False,\n",
        ")\n",
        "\n",
        "def get_output_embeddings():\n",
        "    return model.score\n",
        "\n",
        "model.get_output_embeddings = get_output_embeddings"
      ],
      "metadata": {
        "id": "WVnMGUcwOuEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Configure **LoRA (Low-Rank Adaptation)**, a parameter-efficient finetuning (PEFT) technique. Instead of training all the model's billions of parameters, we only train a small number of \"adapter\" layers."
      ],
      "metadata": {
        "id": "IQMiTsg_O-Ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.score = torch.nn.Linear(1152, 4, bias=False, device=model.device)\n",
        "\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    max_seq_length = max_seq_length,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    r = 32,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 32,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,  # Supports rank stabilized LoRA\n",
        "    task_type = TaskType.SEQ_CLS # Sequence to Classification Task\n",
        ")\n",
        "\n",
        "print(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "trusted": true,
        "id": "Eiifan4mUtHv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune the Model\n",
        "\n",
        "Now, we'll set up the `Trainer` from the HuggingFace's `transformers` library. This class handles the entire training loop, including batching, gradient updates, and logging."
      ],
      "metadata": {
        "id": "OdY7MnbIPKGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"])\n",
        "\n",
        "\n",
        "tokenized_dataset = dataset['train'].map(tokenize_function, batched=True)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    train_dataset = tokenized_dataset,\n",
        "    #eval_dataset = dataset['validation'],\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 8,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        warmup_steps = 10,\n",
        "        learning_rate = 1e-5,\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        num_train_epochs = 1,\n",
        "        report_to = \"none\",\n",
        "        group_by_length = True,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "tQ6fr79zUtHx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Lz7OiY-1UtHy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "Here's an example of the model's prediction on a sample text:"
      ],
      "metadata": {
        "id": "sydT72FAPWim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
        "\n",
        "test_df = dataset['test'].to_pandas()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    text = test_df['text'].iloc[2]\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "    preds = model(**inputs).logits\n",
        "    print(text)\n",
        "    print(preds)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Suk5knPYUtHy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Evaluate the Model\n",
        "\n",
        "Training is done,now it's time to evaluate your model on the test set, which is stored in a Pandas dataframe `test_df`.\n",
        "\n",
        "**Your Task:**\n",
        "1.  Iterate through the test data in batches. A batch size of 16 or 32 is a good choice.\n",
        "2.  For each batch:\n",
        "    *   Tokenize the texts. Make sure to add `padding=True` and `return_tensors=\"pt\"` to get a PyTorch tensor.\n",
        "    *   Move the tokenized inputs to the same device as the model (`model.device`).\n",
        "    *   Get the model's predictions (logits).\n",
        "    *   Find the predicted class for each text by taking the `argmax` of the logits.\n",
        "3.  Keep track of how many predictions are correct.\n",
        "4.  After the loop, calculate and print the final accuracy (in percent).\n",
        "5.  To inspect our predictions, print the `Text`, `True Label`, and `Predicted Label` for the first example in each batch.\n",
        "\n",
        "Fill in the code in the cell below."
      ],
      "metadata": {
        "id": "QqFRR4nBPiWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "# Free some memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Evaluation parameters. You can add more.\n",
        "batch_size = 32\n",
        "num_correct = 0\n",
        "\n",
        "with torch.inference_mode():  # Make predictions in this scope so that you won't accidentally modify the parameters.\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "xmx4p2I1UtHz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Visualizing Model Attention\n",
        "\n",
        "How does the model decide on a classification? **Attention** is a key mechanism. It allows the model to weigh the importance of different words in the input text when making a prediction.\n",
        "\n",
        "By visualizing the attention matrix, we can get a glimpse into the model's \"thought process.\"\n",
        "\n",
        "**Your Task:**\n",
        "1.  Read through the `visualize_attention` helper function provided below. It handles the complex parts of extracting and plotting the attention weights.\n",
        "2.  Write your own text.\n",
        "3.  Call the `visualize_attention` function with your text to see which words the model focuses on."
      ],
      "metadata": {
        "id": "kswmjlamR3v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_attention(text, layer=-1, head=0):\n",
        "    \"\"\"\n",
        "    Visualizes the attention matrix for a given text.\n",
        "\n",
        "    Args:\n",
        "        model: The finetuned model.\n",
        "        tokenizer: The tokenizer.\n",
        "        text (str): The input text to visualize.\n",
        "        layer (int): The model layer to visualize. Default is the last layer.\n",
        "        head (int): The attention head to visualize.\n",
        "    \"\"\"\n",
        "    # To get attention weights, we need to run the model in evaluation mode\n",
        "    # and pass the `output_attentions=True` flag.\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=max_seq_length).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        # Get model outputs, including attention weights\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "\n",
        "    # The `attentions` output is a tuple, one for each layer in the model.\n",
        "    # Each element has shape: [batch_size, num_heads, sequence_length, sequence_length]\n",
        "    attention_matrix = outputs.attentions[layer][0, head].cpu().numpy()\n",
        "\n",
        "    # Get the tokens to use as labels for our plot\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(attention_matrix, xticklabels=tokens, yticklabels=tokens, cmap='viridis')\n",
        "    plt.title(f'Attention Matrix - Layer {layer}, Head {head}')\n",
        "    plt.xlabel('Key/Memory Tokens')\n",
        "    plt.ylabel('Query/Input Tokens')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "CLpniH_qSarI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d68Lu1P_SoGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Conceptual Questions\n",
        "\n",
        "Please answer the following questions in the text cell provided.\n",
        "\n",
        "**Question 1: Pros and Cons of Finetuning**\n",
        "Based on this lab and your understanding, what are the pros and cons of finetuning a large pre-trained model compared to training a smaller model (e.g. logistic regression of SVM) from scratch for a specific task?\n",
        "\n",
        "**Question 2: LoRA Parameters**\n",
        "In Step 3, we configured LoRA with `r=16` and `lora_alpha=16`. Briefly explain the role of these two parameters. What might happen if you set `r` to a very high value (e.g., 256) for this small dataset?\n",
        "\n",
        "**Question 3: LLM Model Choice**\n",
        "Look through a [**list of models here**](https://docs.unsloth.ai/get-started/all-our-models). Name one model that you think might perform well when fine-tuned to the Thai text classification task. Why did you choose this model?"
      ],
      "metadata": {
        "id": "MukYU-D7SwPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1:**\n",
        "\n",
        "\n",
        "\n",
        "**Answer 2:**\n",
        "\n",
        "\n",
        "\n",
        "**Answer 3:**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VtG2G-k3S0zL"
      }
    }
  ]
}