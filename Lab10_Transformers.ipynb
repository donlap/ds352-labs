{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz8bKeFUPYjk"
      },
      "source": [
        "### Install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-OTXMTCKzYr"
      },
      "outputs": [],
      "source": [
        "!python -m pip install transformers accelerate sentencepiece emoji pythainlp --quiet\n",
        "!python -m pip install --no-deps thai2transformers==0.1.2 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls26jNLju8KV"
      },
      "source": [
        "Transformers Documentations: https://huggingface.co/docs/transformers/index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3kN6vKuPbgw"
      },
      "source": [
        "##  Sequence Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yvGSkOsKZGL"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(task=\"sentiment-analysis\",\n",
        "                      model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zZchX9MKtv9"
      },
      "outputs": [],
      "source": [
        "classifier(\"I love to hate you\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8on8vZQ7LcLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A closer look: Tokenization + Classification"
      ],
      "metadata": {
        "id": "0OTSyNJ3Hg07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load tokenizer"
      ],
      "metadata": {
        "id": "D6hthT_5KbZF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoGawT6caf8i"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lxo457Rz3CWW"
      },
      "outputs": [],
      "source": [
        "text = \"I love you\"\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HBBVUFI3Cmu"
      },
      "outputs": [],
      "source": [
        "sentence = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9r1Cs593Csq"
      },
      "outputs": [],
      "source": [
        "sentence = tokenizer(text,  return_tensors=\"pt\")\n",
        "\n",
        "sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model"
      ],
      "metadata": {
        "id": "sxYE_szlKfuN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHqvOXRIcRxn"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification"
      ],
      "metadata": {
        "id": "w3NLWMBuKiEC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkcy48YD3lIn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.softmax(model(**sentence).logits, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from thai2transformers.preprocess import process_transformers\n",
        "\n",
        "input_text = process_transformers(\"ขอเงินกู้<mask>หน่อย<pad>\")\n",
        "\n",
        "thai_classifier = pipeline(task=\"fill-mask\",\n",
        "                           tokenizer=AutoTokenizer.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\"),\n",
        "                           model=\"airesearch/wangchanberta-base-att-spm-uncased\")\n",
        "\n",
        "thai_classifier(input_text)"
      ],
      "metadata": {
        "id": "Xc_jlWArLb4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keYM_TWE6Q15"
      },
      "source": [
        "See an example of the classification model deployed on HuggingFace space at: https://huggingface.co/spaces/Donlapark/sample-text-classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hNllJTdYcwi"
      },
      "source": [
        "# Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaWmO6zgWjz9"
      },
      "outputs": [],
      "source": [
        "!python -m pip install datasets evaluate --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We will fine-tune classification model on the Yelp review dataset"
      ],
      "metadata": {
        "id": "CSfJdApSKk21"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO4ESGFxYMX3"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"yelp_review_full\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLWx4hhPWuTJ"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][100]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modify the tokenizer so that it can be applied to our dataset"
      ],
      "metadata": {
        "id": "3ngZGkrKKsuA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6q8ZBq_XB3v"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\",\n",
        "                                          use_fast=True)\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function,\n",
        "                                 batched=True,\n",
        "                                 remove_columns=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0YvNkRwk98G"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets[\"train\"][100]['input_ids'][:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We will only train on a small subset of the dataset"
      ],
      "metadata": {
        "id": "ZMR44GwrLLv1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iASAnIJ6egDy"
      },
      "outputs": [],
      "source": [
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle().select(range(1000))\n",
        "\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle().select(range(1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model"
      ],
      "metadata": {
        "id": "4WbS8lygLQR0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlckiYvPY8zl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specify training argument"
      ],
      "metadata": {
        "id": "W38IJRbkLRka"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1ppAat2ZDyz"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
        "                                  evaluation_strategy=\"epoch\",\n",
        "                                  learning_rate=2e-5,\n",
        "                                  optim=\"adamw_torch\") ##to use Pytorch's AdamW optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "MlAUnf2uLTto"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RPgO5b8ZRYF"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "\n",
        "    model=model,\n",
        "\n",
        "    args=training_args,\n",
        "\n",
        "    train_dataset=small_train_dataset,\n",
        "\n",
        "    eval_dataset=small_eval_dataset,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lc3m1UmdZSng"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ_9QYtB7Bp3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "sentence = tokenizer(\"I hate you\", return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "torch.softmax(model(**sentence).logits, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA_4eYjKdpSa"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "1. Choose your own task (can be image or audio related) that can be performed using one of the HuggingFace models.\n",
        "2. Use the HugginFace model to create a Streamlit app in a HuggingFace space that asks for the user's input and then perform the said task.\n",
        "3. Deploy the model on HuggingFace space.\n",
        "\n",
        "To see what Transformers can do, you might want to check out the links below:\n",
        "\n",
        "https://huggingface.co/docs/transformers/task_summary\n",
        "\n",
        "https://huggingface.co/docs/transformers/index\n",
        "\n",
        "[List of HuggingFace models](https://huggingface.co/models)\n",
        "\n",
        "[Streamlit Documentation](https://docs.streamlit.io/library/api-reference/widgets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK1aklqY9jrm"
      },
      "source": [
        "#### Insert your HuggingFace Space link here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9VIYIJOpvKw"
      },
      "source": [
        "# Upload model to HuggingFace Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t94lJAfdmp_W"
      },
      "source": [
        "We will upload the tokenizer and the model on HuggingFace hub. First we need to install a library that allows us to log-in our HuggingFace account from colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoJNAlau7Byr"
      },
      "outputs": [],
      "source": [
        "!python -m pip install huggingface_hub --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDPa3RxSm6uW"
      },
      "source": [
        "Enter a credential to login, then create a new model hub, which will be used to store your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNuVmF97k0O6"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login\n",
        "!huggingface-cli repo create finetuned_yelp --type model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7ifozTSnItK"
      },
      "source": [
        "Finally, you can now save your tokenizer and model.\n",
        "\n",
        "To load the mode and tokenizer from the HuggingFace space, use (change `username` to your HuggingFace username):\n",
        "\n",
        "Now you can load the model within HuggingFace Space using `pipeline(\"sentiment-analysis\", model=\"your_username/finetuned_yelp\")`. [Here](https://huggingface.co/spaces/Donlapark/sample-text-classification)'s an example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyL7axtFlxwE"
      },
      "outputs": [],
      "source": [
        "tokenizer.push_to_hub(\"finetuned_yelp\")\n",
        "model.push_to_hub(\"finetuned_yelp\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}