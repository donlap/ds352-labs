{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOVp_RPmLN0N"
   },
   "source": [
    "# DCGAN for Pokemon generation\n",
    "\n",
    "In this lab, you will be training a GAN to make new pokemon sprites. The provided dataset consists of\n",
    "15,467 sprites of all pokemon from generation 1 to generation 8, each of which has size 64 × 64 × 3\n",
    "\n",
    "![sprites](http://www.donlapark.cmustat.com/229352/pokeman.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iuUfFi_o67q3",
    "outputId": "a131923a-e857-4635-aabd-42086daa6949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-Jye5yt9tXP"
   },
   "source": [
    "Download the file: http://www.donlapark.cmustat.com/229352/pokemon.zip and upload it to your google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gIchSU4HZ4LJ",
    "outputId": "f17d7228-919d-411f-ff0d-4173db981643"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/pokemon.zip;\n",
    "!mkdir /content/drive/MyDrive/new_pokemon; # Folder to save images of new pokemon\n",
    "!mkdir /content/drive/MyDrive/GAN_weights # Folder to save images of new pokemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4A6IsrlgaDRn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iyhySZBqzfz"
   },
   "source": [
    "#### Exercise 1: Set the batch size to 16, number of epochs to 80 and latent dimension of the Gaussian noise to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "un055NXUabmX"
   },
   "outputs": [],
   "source": [
    "# TODO-0: set the training parameters below\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "noise_dim = \n",
    "\n",
    "\n",
    "num_examples_to_generate = 16\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "# for normalization\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# preprocessing+augmentation\n",
    "Transform = tt.Compose([\n",
    "    tt.RandomRotation(degrees=15,fill=(255,255,255)),\n",
    "    tt.ColorJitter(hue=0.5),\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize(*stats), #normalize pixels to [-1,1]\n",
    "])\n",
    "\n",
    "\n",
    "data_dir = \"pokemon\"\n",
    "dataset = ImageFolder(data_dir, transform=Transform)\n",
    "dataset.classes = ['pokemon'] #needs to be the name of the subfolder\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size = BATCH_SIZE, \n",
    "                        shuffle = True,\n",
    "                        num_workers = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJMnENHeL3yH"
   },
   "source": [
    "## DCGAN's Generator\n",
    "\n",
    "![generator](https://pytorch.org/tutorials/_images/dcgan_generator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMl5-pE3Gx52"
   },
   "source": [
    "#### Exercise 2: Complete the Generator\n",
    "\n",
    "* The number of channels goes from 512 → 256 → 128 → 64 → 3.\n",
    "* The image size goes from 4 × 4 → 8 × 8 → 16 × 16 → 32 × 32 → 64 × 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5O5Vbn-wahEM"
   },
   "outputs": [],
   "source": [
    "# Generator transforms a Gaussian vector into a 64x64x3 image\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        #define all layers that we need\n",
    "        self.linear = nn.Linear(in_features = noise_dim, \n",
    "                                out_features = 4*4*64*8, \n",
    "                                bias=False)\n",
    "        self.main = nn.Sequential(\n",
    "            # Current shape = 512 x 4 x 4\n",
    "            # See https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n",
    "            nn.ConvTranspose2d(in_channels = 512, \n",
    "                               out_channels = 256, \n",
    "                               kernel_size = 4, \n",
    "                               stride = 2, \n",
    "                               padding = 1, \n",
    "                               bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            # Current shape = 256 x 8 x 8\n",
    "            # TODO-1: fill the rest of the model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh() \n",
    "            # Output shape = 3 x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input): \n",
    "        #build the model using layers that we just defined\n",
    "        x = self.linear(input)\n",
    "        x = x.view(-1, 64*8, 4, 4) #reshape\n",
    "        out = self.main(x)\n",
    "        return out\n",
    "\n",
    "netG = Generator().to(device)\n",
    "\n",
    "# Load the generator to resume training\n",
    "# checkpointG = torch.load('/content/drive/MyDrive/GAN_weights/netG_0020')\n",
    "# netG.load_state_dict(checkpointG['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoiCG4EQMPjm"
   },
   "source": [
    "## New activation function: `LeakyRelu`\n",
    "\n",
    "![leakyrelu](https://d2l.ai/_images/output_dcgan_2541de_111_0.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqn8PQ8FKC3B"
   },
   "source": [
    "#### Exercise 3: Complete the Discriminator\n",
    "\n",
    "* The number of channels goes from 3 → 64 → 128 → 256 → 512.\n",
    "* The image size goes from 64 × 64 → 32 × 32 → 16 × 16 → 8 × 8 → 4 × 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lKl9GabdfVC"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is 3 x 64 x 64\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # Current shape = 64 x 32 x 32\n",
    "            # TODO-2: fill the rest of the model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "# Load the discriminator to resume training\n",
    "# checkpointD = torch.load('/content/drive/MyDrive/GAN_weights/netD_0020')\n",
    "# netD.load_state_dict(checkpointD['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzSu2b_aKKOp"
   },
   "source": [
    "#### Exercise 4: Define the Descriminator's optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UyZ6Lkj_dfp1"
   },
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(num_examples_to_generate, noise_dim, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "# TODO-4: define the optimizers of the discriminator with learning rate = 0.0002 \n",
    "# and betas = (0.5, 0.999)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerD =\n",
    "\n",
    "# Load the optimizers to resume training\n",
    "# optimizerG.load_state_dict(checkpointG['optimizer_state_dict'])\n",
    "# optimizerD.load_state_dict(checkpointD['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIdmnOJLKV0r"
   },
   "source": [
    "#### Exercise 5.1-5.4: Compute the losses using `criterion` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCoyJEpmdfxt"
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(EPOCHS):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_img = data[0].to(device)\n",
    "        real_size = real_img.shape[0]\n",
    "        label = torch.full((real_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_img).view(-1)\n",
    "        ###########################\n",
    "        # TODO-5.1: Calculate the loss between the output and the label\n",
    "        ###########################\n",
    "        errD_real = \n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(real_size, noise_dim, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake_img = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake_img.detach()).view(-1)\n",
    "        ###########################\n",
    "        # TODO-5.2: Calculate the loss between the output and the label\n",
    "        ###########################\n",
    "        errD_fake = \n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        ###########################\n",
    "        # TODO-5.3: Compute the sum of the real and fake losses\n",
    "        ###########################\n",
    "        errD = \n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake_img).view(-1)\n",
    "        ###########################\n",
    "        # TODO-5.4: Calculate the loss between the output and the label\n",
    "        ###########################\n",
    "        errG = \n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, EPOCHS, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    # Check how the generator is doing by saving G's output on fixed_noise\n",
    "    with torch.no_grad():\n",
    "      predictions = netG(fixed_noise).detach().cpu().numpy()\n",
    "      # move colour channel from dim 1 to dim 3 (C,H,W) -> (H,W,C)\n",
    "      predictions = np.moveaxis(predictions,1, 3)\n",
    "\n",
    "      fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "      for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow((predictions[i, :, :, :] +1)/2)\n",
    "        plt.axis('off')\n",
    "\n",
    "      plt.savefig('/content/drive/MyDrive/new_pokemon/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "      torch.save({\n",
    "            'model_state_dict': netG.state_dict(),\n",
    "            'optimizer_state_dict': optimizerG.state_dict()\n",
    "            }, '/content/drive/MyDrive/GAN_weights/netG_{:04d}'.format(epoch))\n",
    "      torch.save({\n",
    "            'model_state_dict': netD.state_dict(),\n",
    "            'optimizer_state_dict': optimizerD.state_dict()\n",
    "            }, '/content/drive/MyDrive/GAN_weights/netD_{:04d}'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNDaSAZbM6Cz"
   },
   "source": [
    "# Extra: Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LAeQckIM8xS",
    "outputId": "a079c347-524d-479b-c927-368d62d03275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 148 kB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!python -m pip install denoising_diffusion_pytorch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbYeiQz2Po5d"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Clear the memory from training DCGAN\n",
    "torch.cuda.empty_cache()\n",
    "del netD, netG, dataloader, fake_img, predictions \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "a4bb8b5579d2497c8231d22b8d2533a4",
      "48c5a0bb55d542da8bfb1d8e1aef16bb",
      "6b046e1dcb7f476394f16bb9f2709c5f",
      "6268edb451274987905c08e1e43bf975",
      "f6c6e423cd5b4a6fa7dcfc5176d4e36e",
      "39894e8c137145d9a0abda25763757de",
      "b8a2370535d14a3194422e7e4f78518d",
      "8677d455f7b6429399001f46e9c7d1a8",
      "75ff6704feb04083a4d49a9141c3de85",
      "d8682c35a7ac4e52a2923523a62a3093",
      "c6758ea358614ae9b2528aeb55252189",
      "c954c489583f438a9e9907656a5ddd21",
      "1ca5ad8f609e4faea2ef8fa3baf21fcb",
      "7cb90f6d301d402bbeea2e5db12bfe35",
      "405179f6d91c46ee963c331b127f9485",
      "effe2b4b384f49c4a0de3d556f76e71f",
      "f3715c4ecc1b48f291a57e52a00d8cc6",
      "f480339edc814e84a8e3a962f2296799",
      "1b6b01efc3fe4a6588280b001a59581f",
      "3eb5145d0994439caa896dca3c9e77c2",
      "2c9293b1433947639e258e0e066d3c8f",
      "776cb14335a2417481e1e1dc10a2155f",
      "288107e6e98943c4a471d4db612d65fa",
      "167dc7dd6b1d484481dafb757f0cc0ae",
      "0f245abd3f7946eface01dd2fda8c087",
      "48bc28ed281049e6954b57e130acd09e",
      "17ef67d18b394392a1261febaa7592f8",
      "455f89fe29404d919b5756fbaa5410d2",
      "4b68f5235f1448d68422f89e5ddea890",
      "15c7e804a67044c0b73689b105aae50d",
      "53829a96abc7491ca4095b82e244a799",
      "6a6547b6ca83497d8d383bff7ef75954",
      "68f5155963cb41508e85091d744d36d4",
      "bd26ce07e6304b1e9718c68f1751a062",
      "7c8c7958dd224641b231868a5da7ee0a",
      "de0e23add13743a1875cdc225070bb4a",
      "18a6d1dc9b4a45b8a38199370463811e",
      "a3f6d7963bce4b8295d38220e96fe5da",
      "677c64f251ad42f39f55929139e7a26d",
      "fd257d222f844ad6bcdf04a4bf80508f",
      "292a41824e4541938c7b78c3e7ac06e3",
      "8b937d88189748f7855f8e696292bb99",
      "5c4e1c4c89a24ad89778b83c47b60421",
      "1dd33babafe64acdbabfc8022705cdc9",
      "e020831f818f452aad82cb73f5c66cbc",
      "1fac1e4df76342709b96c2b856554e53",
      "46743a1897c74305b28550c0d9a80c71",
      "ca220871fb414f57b6a3928f9691fd8f",
      "218d80ae0a8b4c7cb0ca3c8ff7d40ba2",
      "75f32f14e60442069f2750e353b30ea0",
      "fc3f2c4a2e5642a4aa1bfd5040c89f98",
      "fbaef119be0246169aac6a30be9514d8",
      "cc2cd98c892a4b02b582ccfca8eec6b2",
      "a9f2d427bf634e7e8e2d0577b157c2b8",
      "96a198f175664104b9b6c7336c85f823"
     ]
    },
    "id": "MrjBArj1M_7i",
    "outputId": "2a6ea5e9-3df7-4da4-d609-9dcf334768ec"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bb8b5579d2497c8231d22b8d2533a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c954c489583f438a9e9907656a5ddd21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288107e6e98943c4a471d4db612d65fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd26ce07e6304b1e9718c68f1751a062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e020831f818f452aad82cb73f5c66cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n",
    "\n",
    "model = Unet(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8)\n",
    ").cuda()\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = 64,\n",
    "    timesteps = 1000,   # number of steps\n",
    "    loss_type = 'l1'    # L1 or L2\n",
    ").cuda()\n",
    "\n",
    "trainer = Trainer(\n",
    "    diffusion,\n",
    "    'pokemon/pokemon',\n",
    "    train_batch_size = 32,\n",
    "    train_lr = 2e-5,\n",
    "    train_num_steps = 100000,         # total training steps\n",
    "    gradient_accumulate_every = 2,    # gradient accumulation steps\n",
    "    ema_decay = 0.995,                # exponential moving average decay\n",
    "    amp = False                        # turn on mixed precision\n",
    ")\n",
    "\n",
    "# trainer.load('2') # Assume the current epoch is 2\n",
    "# trainer.opt.param_groups[0]['capturable'] = True\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kd22rGk0SuE2"
   },
   "outputs": [],
   "source": [
    "sampled_images = diffusion.sample(batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rLDTpbCNlZ3"
   },
   "outputs": [],
   "source": [
    "imgs = sampled_images.cpu().numpy()\n",
    "for i in range(imgs.shape[0]):\n",
    "  plt.figure()\n",
    "  plt.imshow(imgs[i].reshape(64, 64, 3))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
