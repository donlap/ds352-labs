{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch workflow\n",
        "\n",
        "Reference: https://www.learnpytorch.io/01_pytorch_workflow/\n",
        "\n"
      ],
      "metadata": {
        "id": "410vu1LzvCdo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![workflow](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01_a_pytorch_workflow.png)"
      ],
      "metadata": {
        "id": "e7SkoIa1vIcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Create a dataset"
      ],
      "metadata": {
        "id": "mVEeQtevZMJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the MNIST dataset"
      ],
      "metadata": {
        "id": "ciRJTo4jZQDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "9U3brixEAcjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(X_train[0])\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "id": "VrZOWv9ZGLUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class MyDataSet(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    super(MyDataSet, self).__init__()\n",
        "    self._X = (X/255).astype('float32')\n",
        "    self._y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self._X.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    _X = self._X[idx]\n",
        "    _y = self._y[idx]\n",
        "    return _X, _y"
      ],
      "metadata": {
        "id": "YWfQ21w0uAdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Dataloader` allows us to split the data into minibatches"
      ],
      "metadata": {
        "id": "FoMrjjkeZWpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "dataset = MyDataSet(X_train, y_train)\n",
        "train_set, val_set = torch.utils.data.random_split(dataset, [50000, 10000])\n",
        "\n",
        "trainloader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "valloader = DataLoader(val_set, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "jWPHfIQq60tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Construct a neural network with four layers:\n",
        "* Input layer: 784 nodes\n",
        "* First hidden layer: 512 nodes\n",
        "* Second hidden layer: 512 nodes\n",
        "* Output layer: 10 nodes"
      ],
      "metadata": {
        "id": "_bDk5Wq3ZdEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        # flatten from 2-dim image to a 1-dim vector\n",
        "        self.flatten = nn.Flatten()\n",
        "        # attributes of layers\n",
        "        self.lin1 = nn.Linear(28*28, 512)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.lin2 = nn.Linear(512, 512)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.lin3 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x) # x has shape (64, 28*28)\n",
        "        # define the network using attributes defined above\n",
        "        x = self.lin1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.lin2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.lin3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = SimpleNN()"
      ],
      "metadata": {
        "id": "I9BltSMABBnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Specify the loss function and optimization algorithm"
      ],
      "metadata": {
        "id": "e2zeYf1haG5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Notice that the optimizer stores the model's parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "36RUr2VL4wUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Build training and validation loop"
      ],
      "metadata": {
        "id": "TP4W3c7JaNzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction\n",
        "        pred = model(X)\n",
        "        # Compute loss\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Clear the gradient first\n",
        "        optimizer.zero_grad()\n",
        "        # Compute the gradient with backpropagation\n",
        "        loss.backward()\n",
        "        # Update parameters with the optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def val_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            # Compute prediction\n",
        "            pred = model(X)\n",
        "            # Accumulate the loss\n",
        "            val_loss += loss_fn(pred, y)\n",
        "            correct += (pred.argmax(1) == y).sum().item()\n",
        "\n",
        "    val_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Validation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "w6VVeWs6431H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fit the model to the data"
      ],
      "metadata": {
        "id": "MKy18x3oAGuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(trainloader, model, loss_fn, optimizer)\n",
        "    val_loop(valloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "yUtgq5bGBP8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Make predictions"
      ],
      "metadata": {
        "id": "TfLnkY_ED7DW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_loop(valloader, model, loss_fn)"
      ],
      "metadata": {
        "id": "6jkC1PK1D9jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "\n",
        "Note: you may finish the exercise in a new notebook."
      ],
      "metadata": {
        "id": "y_jglMFiQwzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore the file `CIFAR-10` dataset, which contains the data of images classified into **10 classes**.\n",
        "\n",
        "1. First, split your training set into 80% training set and 20% validation set\n",
        "\n",
        "2. Design your own neural network.\n",
        "\n",
        "3. Try to get your **validation** accuracy as high as possible by trying different network designs, learning rates, batch sizes and epochs.\n",
        "\n",
        "4. Report the accuracy of your final model's predictions on the test set."
      ],
      "metadata": {
        "id": "bXGG1TrQSMUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cifar10"
      ],
      "metadata": {
        "id": "7eVe-Q62EV1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "\n",
        "(train_x, train_y), (test_x, test_y) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "sYZRMII_BMKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(train_x[1])\n",
        "print(train_y[1])"
      ],
      "metadata": {
        "id": "X-0t2JD0EuyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### See the class names [here](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10/load_data#the_classes_are)."
      ],
      "metadata": {
        "id": "jiqAOwrFFxFa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GBdHtuYQHM42"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}